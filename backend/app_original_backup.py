from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import json
import re
from datetime import datetime
import PyPDF2
import dashscope
from config import Config
import jieba
import jieba.posseg as pseg
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from difflib import SequenceMatcher
import math
import logging

# ç¬¬äºŒé˜¶æ®µä¼˜åŒ–ç›¸å…³å¯¼å…¥
from stage2_config import stage2_config, prompt_builder, quality_assessor

# è¯­ä¹‰åµŒå…¥ç›¸å…³å¯¼å…¥
try:
    from sentence_transformers import SentenceTransformer
    import faiss
    EMBEDDING_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: sentence-transformersæˆ–faissæœªå®‰è£…ï¼Œè¯­ä¹‰æœç´¢åŠŸèƒ½å°†è¢«ç¦ç”¨")
    EMBEDDING_AVAILABLE = False

app = Flask(__name__)
app.config.from_object(Config)
CORS(app)

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.getLogger('sentence_transformers').setLevel(logging.WARNING)

# é…ç½®é˜¿é‡Œåƒé—®API
dashscope.api_key = app.config['DASHSCOPE_API_KEY']

# ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
os.makedirs(app.config['KNOWLEDGE_BASE_PATH'], exist_ok=True)

def check_local_model_cache(model_name):
    """æ£€æŸ¥æœ¬åœ°æ˜¯å¦æœ‰æ¨¡å‹ç¼“å­˜"""
    try:
        # Hugging Faceæ¨¡å‹ç¼“å­˜çš„å¯èƒ½ä½ç½®
        cache_locations = [
            os.path.expanduser("~/.cache/huggingface/hub"),
            os.path.expanduser("~/.cache/huggingface/transformers"),
            os.path.expanduser("~/AppData/Local/huggingface/hub"),  # Windows
        ]
        
        model_cache_name = model_name.replace('/', '--')
        
        for cache_dir in cache_locations:
            if not os.path.exists(cache_dir):
                continue
                
            model_dir = os.path.join(cache_dir, f"models--{model_cache_name}")
            if os.path.exists(model_dir):
                snapshots_dir = os.path.join(model_dir, "snapshots")
                if os.path.exists(snapshots_dir):
                    snapshots = os.listdir(snapshots_dir)
                    if snapshots:
                        # æ‰¾åˆ°æœ€æ–°çš„å¿«ç…§
                        latest_snapshot = max(snapshots, key=lambda x: os.path.getctime(os.path.join(snapshots_dir, x)))
                        snapshot_path = os.path.join(snapshots_dir, latest_snapshot)
                        
                        # æ£€æŸ¥å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                        key_files = ['config.json', 'tokenizer_config.json']
                        if all(os.path.exists(os.path.join(snapshot_path, f)) for f in key_files):
                            print(f"âœ“ å‘ç°æœ¬åœ°æ¨¡å‹ç¼“å­˜: {snapshot_path}")
                            return snapshot_path
        
        print(f"Ã— æœªå‘ç°æœ¬åœ°æ¨¡å‹ç¼“å­˜: {model_name}")
        return None
        
    except Exception as e:
        print(f"Ã— æ£€æŸ¥æœ¬åœ°ç¼“å­˜æ—¶å‡ºé”™: {e}")
        return None

def load_embedding_model_smart(model_name, fallback_models=None):
    """æ™ºèƒ½åŠ è½½åµŒå…¥æ¨¡å‹ï¼šä¼˜å…ˆæœ¬åœ°ç¼“å­˜ï¼Œæ— ç¼“å­˜æ—¶åœ¨çº¿åŠ è½½"""
    if fallback_models is None:
        fallback_models = []
    
    all_models = [model_name] + fallback_models
    
    for current_model in all_models:
        try:
            print(f"å°è¯•åŠ è½½æ¨¡å‹: {current_model}")
            
            # 1. æ£€æŸ¥æœ¬åœ°ç¼“å­˜
            local_path = check_local_model_cache(current_model)
            
            if local_path:
                # ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰
                print(f"ä½¿ç”¨æœ¬åœ°ç¼“å­˜åŠ è½½: {current_model}")
                original_offline = os.environ.get('HF_HUB_OFFLINE', '0')
                os.environ['HF_HUB_OFFLINE'] = '1'
                
                try:
                    model = SentenceTransformer(local_path)
                    print(f"âœ“ æˆåŠŸä»æœ¬åœ°ç¼“å­˜åŠ è½½: {current_model}")
                    return model
                except Exception as e:
                    print(f"Ã— æœ¬åœ°ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
                    # å°è¯•ä½¿ç”¨æ¨¡å‹åç§°ä»ç¼“å­˜åŠ è½½
                    try:
                        model = SentenceTransformer(current_model)
                        print(f"âœ“ æˆåŠŸä½¿ç”¨æ¨¡å‹åç§°ä»ç¼“å­˜åŠ è½½: {current_model}")
                        return model
                    except Exception as e2:
                        print(f"Ã— ç¼“å­˜æ¨¡å‹åç§°åŠ è½½ä¹Ÿå¤±è´¥: {e2}")
                finally:
                    # æ¢å¤åŸå§‹ç¦»çº¿è®¾ç½®
                    os.environ['HF_HUB_OFFLINE'] = original_offline
            else:
                # æ²¡æœ‰æœ¬åœ°ç¼“å­˜ï¼Œå°è¯•åœ¨çº¿ä¸‹è½½
                print(f"æœ¬åœ°æ— ç¼“å­˜ï¼Œå°è¯•åœ¨çº¿ä¸‹è½½: {current_model}")
                
                # ç¡®ä¿åœ¨çº¿æ¨¡å¼
                original_offline = os.environ.get('HF_HUB_OFFLINE', '0')
                if 'HF_HUB_OFFLINE' in os.environ:
                    del os.environ['HF_HUB_OFFLINE']
                if 'TRANSFORMERS_OFFLINE' in os.environ:
                    del os.environ['TRANSFORMERS_OFFLINE']
                
                try:
                    model = SentenceTransformer(current_model)
                    print(f"âœ“ æˆåŠŸåœ¨çº¿ä¸‹è½½å¹¶åŠ è½½: {current_model}")
                    return model
                except Exception as e:
                    print(f"Ã— åœ¨çº¿ä¸‹è½½å¤±è´¥: {e}")
                finally:
                    # æ¢å¤åŸå§‹è®¾ç½®
                    if original_offline != '0':
                        os.environ['HF_HUB_OFFLINE'] = original_offline
        
        except Exception as e:
            print(f"Ã— æ¨¡å‹ {current_model} åŠ è½½å®Œå…¨å¤±è´¥: {e}")
            continue
    
    print("Ã— æ‰€æœ‰æ¨¡å‹éƒ½åŠ è½½å¤±è´¥")
    return None

class KnowledgeBase:
    def __init__(self):
        self.documents = []
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.processed_contents = []
        self.filename_patterns = {}  # å­˜å‚¨æ–‡ä»¶åæ¨¡å¼ï¼Œç”¨äºæ™ºèƒ½åŒ¹é…
        
        # è¯­ä¹‰åµŒå…¥ç›¸å…³
        self.embedding_model = None
        self.document_embeddings = None
        self.embedding_index = None
        self.document_chunks = []  # å­˜å‚¨æ–‡æ¡£åˆ†å—ä¿¡æ¯
        
        self.load_knowledge_base()
        # åˆå§‹åŒ–jiebaåˆ†è¯
        jieba.setLogLevel(jieba.logging.INFO)
        self._build_filename_patterns()
        
        # åˆå§‹åŒ–è¯­ä¹‰åµŒå…¥æ¨¡å‹
        if EMBEDDING_AVAILABLE:
            self._init_embedding_model()
        else:
            print("è·³è¿‡è¯­ä¹‰åµŒå…¥æ¨¡å‹åˆå§‹åŒ–")
    
    def load_knowledge_base(self):
        """åŠ è½½çŸ¥è¯†åº“"""
        kb_file = os.path.join(app.config['KNOWLEDGE_BASE_PATH'], 'documents.json')
        if os.path.exists(kb_file):
            try:
                with open(kb_file, 'r', encoding='utf-8') as f:
                    self.documents = json.load(f)
                self._rebuild_search_index()
            except:
                self.documents = []
        else:
            self.documents = []
    
    def _init_embedding_model(self):
        """åˆå§‹åŒ–è¯­ä¹‰åµŒå…¥æ¨¡å‹ - æ™ºèƒ½åŠ è½½ï¼šä¼˜å…ˆæœ¬åœ°ç¼“å­˜ï¼Œæ— ç¼“å­˜æ—¶åœ¨çº¿åŠ è½½"""
        try:
            print("æ­£åœ¨æ™ºèƒ½åŠ è½½è¯­ä¹‰åµŒå…¥æ¨¡å‹...")
            
            # ä¼˜å…ˆä½¿ç”¨çš„æ¨¡å‹åˆ—è¡¨
            preferred_models = [
                'sentence-transformers/all-MiniLM-L6-v2',
                'paraphrase-multilingual-MiniLM-L12-v2',
                'distiluse-base-multilingual-cased'
            ]
            
            # ä½¿ç”¨æ™ºèƒ½åŠ è½½å‡½æ•°
            self.embedding_model = load_embedding_model_smart(
                model_name=preferred_models[0],
                fallback_models=preferred_models[1:]
            )
            
            if self.embedding_model is None:
                print("Ã— æ‰€æœ‰åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨TF-IDFä½œä¸ºå¤‡é€‰")
                return
            
            print("âœ“ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
            # é‡å»ºè¯­ä¹‰ç´¢å¼•
            self._build_semantic_index()
            
        except Exception as e:
            print(f"Ã— åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.embedding_model = None
    
    def _split_document_into_chunks(self, content, chunk_size=300, overlap=50):
        """å°†æ–‡æ¡£åˆ†å‰²æˆè¯­ä¹‰å—"""
        # æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', content)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°å¥å­ä¸ä¼šè¶…è¿‡é™åˆ¶ï¼Œå°±æ·»åŠ 
            if len(current_chunk) + len(sentence) <= chunk_size:
                if current_chunk:
                    current_chunk += "ã€‚" + sentence
                else:
                    current_chunk = sentence
            else:
                # å¦‚æœå½“å‰å—ä¸ä¸ºç©ºï¼Œä¿å­˜å®ƒ
                if current_chunk:
                    chunks.append(current_chunk)
                
                # å¼€å§‹æ–°çš„å—
                current_chunk = sentence
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append(current_chunk)
        
        # å¤„ç†é‡å 
        overlapped_chunks = []
        for i, chunk in enumerate(chunks):
            overlapped_chunks.append(chunk)
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªå—ï¼Œåˆ›å»ºé‡å å—
            if i < len(chunks) - 1 and overlap > 0:
                # å–å½“å‰å—çš„ååŠéƒ¨åˆ†å’Œä¸‹ä¸€å—çš„å‰åŠéƒ¨åˆ†
                chunk_words = list(chunk)
                next_chunk_words = list(chunks[i + 1])
                
                if len(chunk_words) > overlap and len(next_chunk_words) > overlap:
                    overlap_chunk = ''.join(chunk_words[-overlap:]) + ''.join(next_chunk_words[:overlap])
                    if len(overlap_chunk) >= 20:  # åªä¿ç•™æœ‰æ„ä¹‰çš„é‡å å—
                        overlapped_chunks.append(overlap_chunk)
        
        return overlapped_chunks
    
    def _build_semantic_index(self):
        """æ„å»ºè¯­ä¹‰å‘é‡ç´¢å¼•"""
        if not self.embedding_model or not self.documents:
            return
        
        print("æ­£åœ¨æ„å»ºè¯­ä¹‰å‘é‡ç´¢å¼•...")
        
        # æ¸…ç©ºç°æœ‰çš„å—æ•°æ®
        self.document_chunks = []
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºè¯­ä¹‰å—
        all_chunks = []
        for doc in self.documents:
            chunks = self._split_document_into_chunks(doc['content'])
            for chunk in chunks:
                chunk_info = {
                    'text': chunk,
                    'document_id': doc['id'],
                    'filename': doc['filename'],
                    'chunk_index': len(all_chunks)
                }
                self.document_chunks.append(chunk_info)
                all_chunks.append(chunk)
        
        if not all_chunks:
            return
        
        try:
            # ç”ŸæˆåµŒå…¥å‘é‡
            print(f"æ­£åœ¨ä¸º {len(all_chunks)} ä¸ªæ–‡æ¡£å—ç”ŸæˆåµŒå…¥å‘é‡...")
            self.document_embeddings = self.embedding_model.encode(all_chunks, 
                                                                  show_progress_bar=True,
                                                                  normalize_embeddings=True)
            
            # æ„å»ºFAISSç´¢å¼•
            dimension = self.document_embeddings.shape[1]
            self.embedding_index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦
            self.embedding_index.add(self.document_embeddings.astype('float32'))
            
            print(f"âœ“ è¯­ä¹‰ç´¢å¼•æ„å»ºå®Œæˆï¼Œç»´åº¦: {dimension}")
            
        except Exception as e:
            print(f"Ã— è¯­ä¹‰ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            self.embedding_index = None
            self.document_embeddings = None
    
    def _semantic_search(self, query, k=10):
        """æ‰§è¡Œè¯­ä¹‰æœç´¢"""
        if not self.embedding_model or not self.embedding_index:
            return []
        
        try:
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = self.embedding_model.encode([query], normalize_embeddings=True)
            
            # åœ¨ç´¢å¼•ä¸­æœç´¢
            scores, indices = self.embedding_index.search(query_embedding.astype('float32'), k)
            
            # æ„å»ºç»“æœ
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.document_chunks):
                    chunk_info = self.document_chunks[idx]
                    results.append({
                        'chunk_index': idx,
                        'document_id': chunk_info['document_id'],
                        'filename': chunk_info['filename'],
                        'text': chunk_info['text'],
                        'semantic_score': float(score),
                        'rank': i + 1
                    })
            
            return results
            
        except Exception as e:
            print(f"è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return []
    
    def _rerank_results(self, query, semantic_results, keyword_results):
        """é‡æ’åºç»“æœï¼Œèåˆè¯­ä¹‰æœç´¢å’Œå…³é”®è¯æœç´¢çš„ç»“æœ"""
        # åˆ›å»ºç»“æœå­—å…¸ï¼Œä»¥æ–‡æ¡£IDä¸ºé”®
        combined_results = {}
        
        # å¤„ç†è¯­ä¹‰æœç´¢ç»“æœ
        for result in semantic_results:
            doc_id = result['document_id']
            if doc_id not in combined_results:
                combined_results[doc_id] = {
                    'document_id': doc_id,
                    'filename': result['filename'],
                    'semantic_score': 0,
                    'keyword_score': 0,
                    'semantic_chunks': [],
                    'keyword_content': '',
                    'combined_score': 0
                }
            
            # ç´¯ç§¯è¯­ä¹‰åˆ†æ•°ï¼ˆå–å‰3ä¸ªå—çš„å¹³å‡åˆ†ï¼‰
            if len(combined_results[doc_id]['semantic_chunks']) < 3:
                combined_results[doc_id]['semantic_chunks'].append({
                    'text': result['text'],
                    'score': result['semantic_score']
                })
                combined_results[doc_id]['semantic_score'] += result['semantic_score']
        
        # å¤„ç†å…³é”®è¯æœç´¢ç»“æœ
        for result in keyword_results:
            doc_id = result['document_id']
            if doc_id not in combined_results:
                combined_results[doc_id] = {
                    'document_id': doc_id,
                    'filename': result['filename'],
                    'semantic_score': 0,
                    'keyword_score': 0,
                    'semantic_chunks': [],
                    'keyword_content': '',
                    'combined_score': 0
                }
            
            combined_results[doc_id]['keyword_score'] = result['score']
            combined_results[doc_id]['keyword_content'] = result['content']
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°å¹¶æ„å»ºæœ€ç»ˆç»“æœ
        final_results = []
        for doc_id, data in combined_results.items():
            # è¯­ä¹‰åˆ†æ•°å½’ä¸€åŒ–
            semantic_score = data['semantic_score'] / max(len(data['semantic_chunks']), 1)
            keyword_score = data['keyword_score']
            
            # ç»¼åˆè¯„åˆ†ï¼šè¯­ä¹‰æœç´¢æƒé‡0.6ï¼Œå…³é”®è¯æœç´¢æƒé‡0.4
            combined_score = semantic_score * 0.6 + keyword_score * 0.4
            
            # æ„å»ºå†…å®¹
            content_parts = []
            if data['semantic_chunks']:
                # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„è¯­ä¹‰å—
                best_chunks = sorted(data['semantic_chunks'], 
                                   key=lambda x: x['score'], reverse=True)[:2]
                for chunk in best_chunks:
                    content_parts.append(chunk['text'])
            
            if data['keyword_content']:
                content_parts.append(data['keyword_content'])
            
            final_content = '\n\n'.join(content_parts)
            
            final_results.append({
                'document_id': doc_id,
                'filename': data['filename'],
                'content': final_content,
                'score': combined_score,
                'semantic_score': semantic_score,
                'keyword_score': keyword_score,
                'reranked': True
            })
        
        # æŒ‰ç»¼åˆåˆ†æ•°æ’åº
        final_results.sort(key=lambda x: x['score'], reverse=True)
        return final_results
    
    def save_knowledge_base(self):
        """ä¿å­˜çŸ¥è¯†åº“"""
        kb_file = os.path.join(app.config['KNOWLEDGE_BASE_PATH'], 'documents.json')
        with open(kb_file, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)
    
    def _preprocess_text(self, text):
        """æ–‡æœ¬é¢„å¤„ç†ï¼šæ¸…æ´—ã€åˆ†è¯ã€å»åœç”¨è¯"""
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'\s+', ' ', text)  # è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)  # åªä¿ç•™ä¸­è‹±æ–‡æ•°å­—
        
        # ä½¿ç”¨jiebaåˆ†è¯
        words = jieba.lcut(text)
        
        # ç®€å•çš„åœç”¨è¯åˆ—è¡¨
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä¸º', 'ä¸', 'ç­‰', 'å¯ä»¥', 'å¦‚ä½•', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ'}
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        filtered_words = [word for word in words if len(word) > 1 and word not in stop_words]
        
        return ' '.join(filtered_words)
    
    def _extract_keywords(self, text, top_k=10):
        """æå–å…³é”®è¯"""
        words = pseg.cut(text)
        # é€‰æ‹©åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ä½œä¸ºå…³é”®è¯
        keywords = []
        for word, flag in words:
            if flag.startswith(('n', 'v', 'a')) and len(word) > 1:
                keywords.append(word)
        
        # å»é‡å¹¶è¿”å›å‰top_kä¸ª
        return list(set(keywords))[:top_k]
    
    def _rebuild_search_index(self):
        """é‡å»ºæœç´¢ç´¢å¼•"""
        if not self.documents:
            return
        
        # é¢„å¤„ç†æ‰€æœ‰æ–‡æ¡£å†…å®¹
        self.processed_contents = []
        for doc in self.documents:
            processed_content = self._preprocess_text(doc['content'])
            self.processed_contents.append(processed_content)
        
        # æ„å»ºTF-IDFå‘é‡
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 2),  # æ”¯æŒ1-2gram
            min_df=1,
            max_df=0.95
        )
        
        try:
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.processed_contents)
        except:
            self.tfidf_matrix = None
    
    def _build_filename_patterns(self):
        """æ„å»ºæ–‡ä»¶ååŒ¹é…æ¨¡å¼ï¼Œæ”¯æŒå¤šç§æ–‡ä»¶åè¯†åˆ«æ–¹å¼"""
        self.filename_patterns = {}
        for doc in self.documents:
            filename = doc['filename']
            # å­˜å‚¨å®Œæ•´æ–‡ä»¶åï¼ˆåŒ…å«æ‰©å±•åï¼‰
            self.filename_patterns[filename.lower()] = doc['id']
            
            # å­˜å‚¨ä¸å«æ‰©å±•åçš„æ–‡ä»¶å
            name_without_ext = os.path.splitext(filename)[0].lower()
            self.filename_patterns[name_without_ext] = doc['id']
            
            # å¤„ç†ä¸­æ–‡æ–‡ä»¶åçš„å…³é”®è¯
            keywords = self._extract_filename_keywords(filename)
            for keyword in keywords:
                if keyword not in self.filename_patterns:
                    self.filename_patterns[keyword] = []
                if isinstance(self.filename_patterns[keyword], list):
                    if doc['id'] not in self.filename_patterns[keyword]:
                        self.filename_patterns[keyword].append(doc['id'])
                else:
                    # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
                    existing_id = self.filename_patterns[keyword]
                    self.filename_patterns[keyword] = [existing_id, doc['id']]
    
    def _extract_filename_keywords(self, filename):
        """ä»æ–‡ä»¶åä¸­æå–å…³é”®è¯"""
        # å»æ‰æ‰©å±•å
        name_without_ext = os.path.splitext(filename)[0]
        
        # ä½¿ç”¨jiebaåˆ†è¯æå–ä¸­æ–‡å…³é”®è¯
        keywords = []
        words = jieba.lcut(name_without_ext)
        for word in words:
            if len(word) > 1 and word not in {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'ä¸', 'ç­‰'}:
                keywords.append(word.lower())
        
        # æ·»åŠ è‹±æ–‡å•è¯ï¼ˆæŒ‰ç©ºæ ¼ã€ä¸‹åˆ’çº¿ã€æ¨ªçº¿åˆ†å‰²ï¼‰
        import re
        english_words = re.findall(r'[a-zA-Z]+', name_without_ext)
        for word in english_words:
            if len(word) > 2:
                keywords.append(word.lower())
        
        return keywords
    
    def _detect_target_filename(self, query):
        """æ£€æµ‹æŸ¥è¯¢ä¸­æ˜¯å¦åŒ…å«ç‰¹å®šçš„æ–‡ä»¶å"""
        query_lower = query.lower()
        detected_files = []
        confidence_scores = {}
        
        # 1. å®Œå…¨åŒ¹é…æ£€æµ‹
        for pattern, doc_ids in self.filename_patterns.items():
            if pattern in query_lower:
                if isinstance(doc_ids, list):
                    for doc_id in doc_ids:
                        if doc_id not in detected_files:
                            detected_files.append(doc_id)
                            confidence_scores[doc_id] = confidence_scores.get(doc_id, 0) + len(pattern) * 2
                else:
                    if doc_ids not in detected_files:
                        detected_files.append(doc_ids)
                        confidence_scores[doc_ids] = confidence_scores.get(doc_ids, 0) + len(pattern) * 2
        
        # 2. æ–‡ä»¶æ‰©å±•åæ£€æµ‹ï¼ˆå¦‚æœç”¨æˆ·æåˆ°äº†.pdf, .docç­‰ï¼‰
        extensions = ['.pdf', '.doc', '.docx', '.txt', '.xlsx']
        for ext in extensions:
            if ext in query_lower:
                # å¯»æ‰¾æ‰©å±•åå‰çš„æ–‡ä»¶åéƒ¨åˆ†
                import re
                pattern = r'(\S+)' + re.escape(ext)
                matches = re.finditer(pattern, query_lower)
                for match in matches:
                    potential_filename = match.group(1) + ext
                    for doc in self.documents:
                        if doc['filename'].lower() == potential_filename:
                            if doc['id'] not in detected_files:
                                detected_files.append(doc['id'])
                                confidence_scores[doc['id']] = confidence_scores.get(doc['id'], 0) + 10
        
        # 3. æŒ‰ç½®ä¿¡åº¦æ’åº
        if detected_files and confidence_scores:
            detected_files.sort(key=lambda x: confidence_scores.get(x, 0), reverse=True)
        
        return detected_files, confidence_scores
    
    def _fuzzy_match_score(self, query_word, doc_words):
        """è®¡ç®—æ¨¡ç³ŠåŒ¹é…åˆ†æ•°"""
        max_score = 0
        for word in doc_words:
            # ä½¿ç”¨åºåˆ—åŒ¹é…è®¡ç®—ç›¸ä¼¼åº¦
            similarity = SequenceMatcher(None, query_word, word).ratio()
            if similarity > max_score:
                max_score = similarity
        return max_score
    
    def _calculate_keyword_match_score(self, query_keywords, doc_content):
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        content_lower = doc_content.lower()
        doc_words = jieba.lcut(content_lower)
        
        total_score = 0
        matched_keywords = 0
        
        for keyword in query_keywords:
            keyword_lower = keyword.lower()
            
            # ç²¾ç¡®åŒ¹é…
            if keyword_lower in content_lower:
                total_score += 2.0
                matched_keywords += 1
            else:
                # æ¨¡ç³ŠåŒ¹é…
                fuzzy_score = self._fuzzy_match_score(keyword_lower, doc_words)
                if fuzzy_score > 0.7:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    total_score += fuzzy_score
                    matched_keywords += 1
        
        # è€ƒè™‘åŒ¹é…å…³é”®è¯çš„æ¯”ä¾‹
        if query_keywords:
            match_ratio = matched_keywords / len(query_keywords)
            total_score *= (1 + match_ratio)
        
        return total_score
    
    def _extract_relevant_snippets(self, content, query_keywords, max_snippets=3, snippet_length=200):
        """æå–ç›¸å…³æ–‡æœ¬ç‰‡æ®µ"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', content)
        scored_sentences = []
        
        for sentence in sentences:
            if len(sentence.strip()) < 10:  # è·³è¿‡è¿‡çŸ­çš„å¥å­
                continue
                
            score = 0
            sentence_lower = sentence.lower()
            
            # è®¡ç®—å¥å­ä¸æŸ¥è¯¢å…³é”®è¯çš„ç›¸å…³æ€§
            for keyword in query_keywords:
                if keyword.lower() in sentence_lower:
                    score += 1
                    # å¦‚æœå…³é”®è¯åœ¨å¥å­å¼€å¤´ï¼Œç»™äºˆé¢å¤–åˆ†æ•°
                    if sentence_lower.strip().startswith(keyword.lower()):
                        score += 0.5
            
            if score > 0:
                scored_sentences.append((sentence.strip(), score))
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶é€‰æ‹©æœ€ç›¸å…³çš„ç‰‡æ®µ
        scored_sentences.sort(key=lambda x: x[1], reverse=True)
        
        snippets = []
        for sentence, _ in scored_sentences[:max_snippets]:
            # ç¡®ä¿ç‰‡æ®µä¸è¶…è¿‡æŒ‡å®šé•¿åº¦
            if len(sentence) > snippet_length:
                sentence = sentence[:snippet_length] + "..."
            snippets.append(sentence)
        
        return snippets
    
    def add_document(self, filename, content):
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        doc = {
            'id': len(self.documents) + 1,
            'filename': filename,
            'content': content,
            'created_at': datetime.now().isoformat()
        }
        self.documents.append(doc)
        self.save_knowledge_base()
        # é‡å»ºæœç´¢ç´¢å¼•å’Œæ–‡ä»¶åæ¨¡å¼
        self._rebuild_search_index()
        self._build_filename_patterns()
        
        # é‡å»ºè¯­ä¹‰ç´¢å¼•
        if EMBEDDING_AVAILABLE and self.embedding_model:
            self._build_semantic_index()
        
        return doc['id']
    
    def delete_document(self, doc_id):
        """åˆ é™¤çŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£"""
        try:
            # 1. æŸ¥æ‰¾è¦åˆ é™¤çš„æ–‡æ¡£
            doc_to_delete = None
            for i, doc in enumerate(self.documents):
                if doc['id'] == doc_id:
                    doc_to_delete = doc
                    break
            
            if not doc_to_delete:
                print(f"æ–‡æ¡£ ID {doc_id} ä¸å­˜åœ¨")
                return False
            
            # 2. ä»æ–‡æ¡£åˆ—è¡¨ä¸­ç§»é™¤
            self.documents = [doc for doc in self.documents if doc['id'] != doc_id]
            
            # 3. åˆ é™¤ç›¸åº”çš„ç‰©ç†æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            try:
                file_path = os.path.join(app.config['UPLOAD_FOLDER'], doc_to_delete['filename'])
                if os.path.exists(file_path):
                    os.remove(file_path)
                    print(f"å·²åˆ é™¤ç‰©ç†æ–‡ä»¶: {file_path}")
            except Exception as e:
                print(f"åˆ é™¤ç‰©ç†æ–‡ä»¶å¤±è´¥: {e}")
                # å³ä½¿ç‰©ç†æ–‡ä»¶åˆ é™¤å¤±è´¥ï¼Œä¹Ÿç»§ç»­åˆ é™¤æ•°æ®åº“è®°å½•
            
            # 4. ä¿å­˜æ›´æ–°åçš„çŸ¥è¯†åº“
            self.save_knowledge_base()
            
            # 5. é‡å»ºæœç´¢ç´¢å¼•
            self._rebuild_search_index()
            
            # 6. é‡å»ºæ–‡ä»¶åæ¨¡å¼
            self._build_filename_patterns()
            
            # 7. é‡å»ºè¯­ä¹‰ç´¢å¼•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if EMBEDDING_AVAILABLE and self.embedding_model:
                try:
                    self._build_semantic_index()
                    print("è¯­ä¹‰ç´¢å¼•å·²é‡å»º")
                except Exception as e:
                    print(f"é‡å»ºè¯­ä¹‰ç´¢å¼•å¤±è´¥: {e}")
            
            print(f"æ–‡æ¡£ '{doc_to_delete['filename']}' (ID: {doc_id}) å·²æˆåŠŸåˆ é™¤")
            return True
            
        except Exception as e:
            print(f"åˆ é™¤æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False

    def search(self, query, threshold=0.1, max_results=5, target_documents=None):
        """å¢å¼ºçš„æ™ºèƒ½æœç´¢åŠŸèƒ½ï¼Œæ”¯æŒè¯­ä¹‰æœç´¢å’Œé‡æ’åº"""
        if not self.documents:
            return []
        
        # 1. æ£€æµ‹æŸ¥è¯¢ä¸­æ˜¯å¦æŒ‡å‘ç‰¹å®šæ–‡æ¡£
        detected_files, confidence_scores = self._detect_target_filename(query)
        
        # å¦‚æœæ£€æµ‹åˆ°ç‰¹å®šæ–‡æ¡£å¼•ç”¨ï¼Œé™åˆ¶æœç´¢èŒƒå›´
        search_scope = target_documents or detected_files
        
        results = []
        
        # 2. é¢„å¤„ç†æŸ¥è¯¢
        processed_query = self._preprocess_text(query)
        query_keywords = self._extract_keywords(query)
        
        # 3. ç¡®å®šè¦æœç´¢çš„æ–‡æ¡£èŒƒå›´
        documents_to_search = []
        if search_scope:
            # åªåœ¨æŒ‡å®šçš„æ–‡æ¡£ä¸­æœç´¢
            for doc in self.documents:
                if doc['id'] in search_scope:
                    documents_to_search.append(doc)
            search_info = {
                'search_mode': 'targeted',
                'target_files': [doc['filename'] for doc in documents_to_search],
                'detection_confidence': confidence_scores,
                'semantic_enabled': EMBEDDING_AVAILABLE and self.embedding_model is not None
            }
        else:
            # åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­æœç´¢
            documents_to_search = self.documents
            search_info = {
                'search_mode': 'global',
                'target_files': [],
                'detection_confidence': {},
                'semantic_enabled': EMBEDDING_AVAILABLE and self.embedding_model is not None
            }
        
        if not documents_to_search:
            return []
        
        # 4. è¯­ä¹‰æœç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        semantic_results = []
        if EMBEDDING_AVAILABLE and self.embedding_model and self.embedding_index:
            try:
                semantic_results = self._semantic_search(query, k=max_results * 2)
                
                # å¦‚æœæ˜¯é’ˆå¯¹ç‰¹å®šæ–‡æ¡£çš„æœç´¢ï¼Œè¿‡æ»¤è¯­ä¹‰ç»“æœ
                if search_scope:
                    semantic_results = [r for r in semantic_results 
                                      if r['document_id'] in search_scope]
                
                print(f"è¯­ä¹‰æœç´¢æ‰¾åˆ° {len(semantic_results)} ä¸ªç»“æœ")
            except Exception as e:
                print(f"è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
                semantic_results = []
        
        # 5. ä¼ ç»Ÿå…³é”®è¯æœç´¢ï¼ˆä½œä¸ºå¤‡é€‰å’Œè¡¥å……ï¼‰
        keyword_results = []
        
        # TF-IDFå‘é‡æœç´¢ï¼ˆå¦‚æœå¯ç”¨ä¸”åœ¨å…¨å±€æœç´¢æ¨¡å¼ï¼‰
        tfidf_scores = {}
        if self.tfidf_matrix is not None and processed_query and not search_scope:
            try:
                query_vector = self.tfidf_vectorizer.transform([processed_query])
                similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()
                
                for i, score in enumerate(similarities):
                    if score > threshold and i < len(self.documents):
                        tfidf_scores[self.documents[i]['id']] = score
            except:
                pass
        
        # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
        for doc in documents_to_search:
            total_score = 0
            
            # TF-IDFåˆ†æ•°ï¼ˆåªåœ¨å…¨å±€æœç´¢æ—¶ä½¿ç”¨ï¼‰
            tfidf_score = tfidf_scores.get(doc['id'], 0) if not search_scope else 0
            total_score += tfidf_score * 2  # é™ä½TF-IDFæƒé‡
            
            # å…³é”®è¯åŒ¹é…åˆ†æ•°
            keyword_score = self._calculate_keyword_match_score(query_keywords, doc['content'])
            total_score += keyword_score * 1.5  # è°ƒæ•´å…³é”®è¯åŒ¹é…æƒé‡
            
            # ç®€å•æ–‡æœ¬åŒ¹é…åˆ†æ•°ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
            query_lower = query.lower()
            content_lower = doc['content'].lower()
            if query_lower in content_lower:
                total_score += len(re.findall(re.escape(query_lower), content_lower)) * 0.3
            
            # å¦‚æœæ˜¯é’ˆå¯¹ç‰¹å®šæ–‡æ¡£çš„æœç´¢ï¼Œé™ä½é˜ˆå€¼å¹¶ç»™äºˆé¢å¤–åˆ†æ•°
            if search_scope and doc['id'] in search_scope:
                total_score += confidence_scores.get(doc['id'], 0) * 0.1
                effective_threshold = threshold * 0.3
            else:
                effective_threshold = threshold
            
            # å¦‚æœæ€»åˆ†æ•°è¶…è¿‡é˜ˆå€¼ï¼Œæ·»åŠ åˆ°å…³é”®è¯ç»“æœ
            if total_score > effective_threshold:
                relevant_snippets = self._extract_relevant_snippets(
                    doc['content'], 
                    query_keywords + [query],
                    max_snippets=2
                )
                
                if relevant_snippets:
                    keyword_results.append({
                        'document_id': doc['id'],
                        'filename': doc['filename'],
                        'content': '\n'.join(relevant_snippets),
                        'score': total_score,
                        'tfidf_score': tfidf_score,
                        'keyword_score': keyword_score,
                        'matched_keywords': query_keywords,
                        'search_info': search_info,
                        'file_match_confidence': confidence_scores.get(doc['id'], 0)
                    })
        
        # 6. ç»“æœèåˆå’Œé‡æ’åº
        if semantic_results and keyword_results:
            # ä½¿ç”¨é‡æ’åºç®—æ³•èåˆç»“æœ
            try:
                results = self._rerank_results(query, semantic_results, keyword_results)
                for result in results:
                    result['search_info'] = search_info
                print(f"é‡æ’åºåå¾—åˆ° {len(results)} ä¸ªç»“æœ")
            except Exception as e:
                print(f"é‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯ç»“æœ: {e}")
                results = keyword_results
                
        elif semantic_results:
            # åªæœ‰è¯­ä¹‰æœç´¢ç»“æœ
            print("åªä½¿ç”¨è¯­ä¹‰æœç´¢ç»“æœ")
            doc_results = {}
            for sem_result in semantic_results:
                doc_id = sem_result['document_id']
                if doc_id not in doc_results:
                    doc_results[doc_id] = {
                        'document_id': doc_id,
                        'filename': sem_result['filename'],
                        'content': sem_result['text'],
                        'score': sem_result['semantic_score'],
                        'semantic_score': sem_result['semantic_score'],
                        'keyword_score': 0,
                        'search_info': search_info,
                        'file_match_confidence': confidence_scores.get(doc_id, 0)
                    }
                else:
                    # åˆå¹¶å¤šä¸ªè¯­ä¹‰å—
                    doc_results[doc_id]['content'] += '\n\n' + sem_result['text']
                    doc_results[doc_id]['score'] = max(doc_results[doc_id]['score'], 
                                                     sem_result['semantic_score'])
            
            results = list(doc_results.values())
            results.sort(key=lambda x: x['score'], reverse=True)
            
        else:
            # åªæœ‰å…³é”®è¯æœç´¢ç»“æœ
            print("åªä½¿ç”¨å…³é”®è¯æœç´¢ç»“æœ")
            results = keyword_results
        
        # 7. å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æœä¸”ä¸æ˜¯é’ˆå¯¹ç‰¹å®šæ–‡æ¡£çš„æœç´¢ï¼Œé™ä½é˜ˆå€¼é‡æ–°æœç´¢
        if not results and not search_scope and threshold > 0.05:
            print("é™ä½é˜ˆå€¼é‡æ–°æœç´¢...")
            return self.search(query, threshold=threshold * 0.5, max_results=max_results)
        
        # 8. æŒ‰ç»¼åˆåˆ†æ•°æ’åºå¹¶è¿”å›ç»“æœ
        if not any('reranked' in result for result in results):
            results.sort(key=lambda x: x['score'], reverse=True)
        
        return results[:max_results]
    
    def _build_general_prompt(self, context, user_question):
        """æ„å»ºé€šç”¨RAGæç¤ºè¯æ¡†æ¶ - ä½¿ç”¨é…ç½®åŒ–çš„ç¬¬äºŒé˜¶æ®µä¼˜åŒ–ç‰ˆæœ¬"""
        return prompt_builder.build_system_prompt(
            prompt_type="general",
            context=context,
            user_question=user_question
        )
    
    def _build_targeted_prompt(self, context, target_files, user_question):
        """æ„å»ºé’ˆå¯¹ç‰¹å®šæ–‡æ¡£çš„RAGæç¤ºè¯æ¡†æ¶ - ä½¿ç”¨é…ç½®åŒ–çš„ç¬¬äºŒé˜¶æ®µä¼˜åŒ–ç‰ˆæœ¬"""
        return prompt_builder.build_system_prompt(
            prompt_type="targeted",
            context=context,
            user_question=user_question,
            target_files=target_files
        )
    
    def _enhance_context_quality(self, search_results, user_question):
        """å¢å¼ºä¸Šä¸‹æ–‡è´¨é‡ - ç¬¬äºŒé˜¶æ®µä¼˜åŒ–"""
        if not search_results:
            return "", []
        
        # 1. æŒ‰ç›¸å…³æ€§é‡æ–°æ’åº
        enhanced_results = []
        for result in search_results:
            # è®¡ç®—å†…å®¹ä¸é—®é¢˜çš„ç›¸å…³æ€§
            content_relevance = self._calculate_content_relevance(result['content'], user_question)
            result['enhanced_score'] = result.get('score', 0) + content_relevance * 0.3
            enhanced_results.append(result)
        
        # 2. é‡æ–°æ’åº
        enhanced_results.sort(key=lambda x: x['enhanced_score'], reverse=True)
        
        # 3. æ„å»ºé«˜è´¨é‡ä¸Šä¸‹æ–‡
        context_parts = []
        source_files = []
        
        for i, result in enumerate(enhanced_results):
            source_files.append(result['filename'])
            
            # ä¸ºæ¯ä¸ªå†…å®¹ç‰‡æ®µæ·»åŠ æ›´æ¸…æ™°çš„æ ‡è¯†
            content_snippet = f"""
**æ–‡æ¡£æ¥æºï¼š{result['filename']}**
**ç›¸å…³æ€§è¯„åˆ†ï¼š{result['enhanced_score']:.3f}**
**å†…å®¹ï¼š**
{result['content'].strip()}
---"""
            context_parts.append(content_snippet)
        
        enhanced_context = "\n".join(context_parts)
        
        return enhanced_context, list(set(source_files))
    
    def _calculate_content_relevance(self, content, question):
        """è®¡ç®—å†…å®¹ä¸é—®é¢˜çš„ç›¸å…³æ€§"""
        # ç®€å•çš„å…³é”®è¯åŒ¹é…ç›¸å…³æ€§è®¡ç®—
        question_words = set(jieba.lcut(question.lower()))
        content_words = set(jieba.lcut(content.lower()))
        
        if not question_words:
            return 0
        
        # è®¡ç®—äº¤é›†æ¯”ä¾‹
        intersection = len(question_words.intersection(content_words))
        relevance = intersection / len(question_words)
        
        return relevance

# åˆå§‹åŒ–çŸ¥è¯†åº“
kb = KnowledgeBase()

def extract_text_from_pdf(file_path):
    """ä»PDFæ–‡ä»¶æå–æ–‡æœ¬"""
    text = ""
    try:
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page in pdf_reader.pages:
                text += page.extract_text()
    except Exception as e:
        print(f"PDFæå–é”™è¯¯: {e}")
    return text

def call_qianwen_api(messages):
    """è°ƒç”¨é˜¿é‡Œåƒé—®API"""
    try:
        from dashscope import Generation
        
        response = Generation.call(
            model='qwen-turbo',
            messages=messages,
            result_format='message'
        )
        
        if response.status_code == 200:
            return response.output.choices[0].message.content
        else:
            return f"APIè°ƒç”¨å¤±è´¥: {response.message}"
    except Exception as e:
        return f"APIè°ƒç”¨é”™è¯¯: {str(e)}"

@app.route('/api/chat', methods=['POST'])
def chat():
    """èŠå¤©æ¥å£ - æ”¯æŒæ™ºèƒ½æ–‡æ¡£æ£€ç´¢ - ç¬¬äºŒé˜¶æ®µRAGä¼˜åŒ–ç‰ˆæœ¬"""
    data = request.json
    user_message = data.get('message', '')
    
    if not user_message:
        return jsonify({'error': 'æ¶ˆæ¯ä¸èƒ½ä¸ºç©º'}), 400
    
    # å…ˆåœ¨çŸ¥è¯†åº“ä¸­æœç´¢ï¼ˆä¼šè‡ªåŠ¨æ£€æµ‹æ˜¯å¦é’ˆå¯¹ç‰¹å®šæ–‡æ¡£ï¼‰
    search_results = kb.search(user_message)
    
    if search_results:
        # ä½¿ç”¨å¢å¼ºçš„ä¸Šä¸‹æ–‡è´¨é‡ä¼˜åŒ–
        enhanced_context, source_files = kb._enhance_context_quality(search_results, user_message)
        search_mode = search_results[0].get('search_info', {}).get('search_mode', 'global')
        
        # æ ¹æ®æœç´¢æ¨¡å¼è°ƒæ•´ç³»ç»Ÿæç¤ºè¯ - ä½¿ç”¨ä¼˜åŒ–çš„æç¤ºè¯æ¡†æ¶
        if search_mode == 'targeted':
            target_files = search_results[0].get('search_info', {}).get('target_files', [])
            system_prompt = kb._build_targeted_prompt(enhanced_context, target_files, user_message)
        else:
            system_prompt = kb._build_general_prompt(enhanced_context, user_message)
        
        messages = [
            {'role': 'system', 'content': system_prompt},
            {'role': 'user', 'content': user_message}
        ]
        
        response = call_qianwen_api(messages)
        
        # è¯„ä¼°å›ç­”è´¨é‡
        quality_assessment = quality_assessor.assess_response_quality(response)
        
        return jsonify({
            'response': response,
            'source': 'knowledge_base',
            'search_results': search_results,
            'search_mode': search_mode,
            'source_files': source_files,
            'optimization_stage': 'stage2_prompt_optimization',  # æ ‡è¯†ä½¿ç”¨äº†ç¬¬äºŒé˜¶æ®µä¼˜åŒ–
            'quality_assessment': quality_assessment,
            'prompt_version': 'v2.0'
        })
    else:
        # å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨æ ‡å‡†å›å¤æ¨¡æ¿
        fallback_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚ç”¨æˆ·è¯¢é—®çš„é—®é¢˜åœ¨å½“å‰çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚

è¯·å›ç­”ï¼šå¯¹ä¸èµ·ï¼Œæˆ‘åœ¨å½“å‰çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚æ‚¨å¯ä»¥ï¼š
1. å°è¯•é‡æ–°æè¿°æ‚¨çš„é—®é¢˜
2. ä¸Šä¼ ç›¸å…³æ–‡æ¡£åˆ°çŸ¥è¯†åº“
3. ä½¿ç”¨ä¸åŒçš„å…³é”®è¯è¿›è¡Œè¯¢é—®

å¦‚æœæ‚¨å¸Œæœ›æˆ‘åŸºäºé€šç”¨çŸ¥è¯†å›ç­”ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ã€‚"""
        
        messages = [
            {'role': 'system', 'content': fallback_prompt},
            {'role': 'user', 'content': user_message}
        ]
        
        response = call_qianwen_api(messages)
        
        # ä¸ºæ— åŒ¹é…æƒ…å†µä¹Ÿè¿›è¡Œè´¨é‡è¯„ä¼°
        quality_assessment = quality_assessor.assess_response_quality(response)
        
        return jsonify({
            'response': response,
            'source': 'no_knowledge_base_match',
            'search_results': [],
            'search_mode': 'none',
            'source_files': [],
            'optimization_stage': 'stage2_prompt_optimization',
            'quality_assessment': quality_assessment,
            'prompt_version': 'v2.0'
        })

@app.route('/api/upload', methods=['POST'])
def upload_file():
    """æ–‡ä»¶ä¸Šä¼ æ¥å£"""
    if 'file' not in request.files:
        return jsonify({'error': 'æ²¡æœ‰æ–‡ä»¶'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
    
    if file:
        filename = file.filename
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(file_path)
        
        # æå–æ–‡æœ¬å†…å®¹
        content = ""
        if filename.lower().endswith('.pdf'):
            content = extract_text_from_pdf(file_path)
        elif filename.lower().endswith(('.txt', '.md')):
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        
        if content:
            # æ·»åŠ åˆ°çŸ¥è¯†åº“
            kb.add_document(filename, content)
            return jsonify({'message': f'æ–‡ä»¶ {filename} ä¸Šä¼ å¹¶å¤„ç†æˆåŠŸ'})
        else:
            return jsonify({'error': 'æ— æ³•æå–æ–‡ä»¶å†…å®¹'}), 400

@app.route('/api/search', methods=['POST'])
def search_knowledge_base():
    """çŸ¥è¯†åº“æœç´¢æ¥å£"""
    data = request.json
    query = data.get('query', '')
    max_results = data.get('max_results', 5)
    
    if not query:
        return jsonify({'error': 'æŸ¥è¯¢ä¸èƒ½ä¸ºç©º'}), 400
    
    results = kb.search(query, max_results=max_results)
    
    return jsonify({
        'results': results,
        'query': query,
        'total_documents': len(kb.documents)
    })

@app.route('/api/documents', methods=['GET'])
def get_documents():
    """è·å–çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£"""
    documents = [
        {
            'id': doc['id'],
            'filename': doc['filename'],
            'upload_time': doc.get('upload_time', ''),
            'content_length': len(doc['content'])
        }
        for doc in kb.documents
    ]
    
    return jsonify({
        'documents': documents,
        'total': len(documents)
    })

@app.route('/api/documents/<int:doc_id>', methods=['DELETE'])
def delete_document(doc_id):
    """åˆ é™¤çŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£"""
    if kb.delete_document(doc_id):
        return jsonify({'message': 'æ–‡æ¡£åˆ é™¤æˆåŠŸ'})
    else:
        return jsonify({'error': 'æ–‡æ¡£ä¸å­˜åœ¨'}), 404

@app.route('/api/search_comparison', methods=['POST'])
def search_comparison():
    """æœç´¢æ–¹æ³•å¯¹æ¯”æ¥å£"""
    data = request.json
    query = data.get('query', '')
    
    if not query:
        return jsonify({'error': 'æŸ¥è¯¢ä¸èƒ½ä¸ºç©º'}), 400
    
    # ä½¿ç”¨ä¸åŒæ–¹æ³•æœç´¢
    keyword_results = kb._keyword_search(query)
    semantic_results = []
    
    if EMBEDDING_AVAILABLE and kb.embedding_model:
        semantic_results = kb._semantic_search(query)
    
    return jsonify({
        'keyword_results': keyword_results,
        'semantic_results': semantic_results,
        'query': query
    })

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({
        'status': 'healthy',
        'knowledge_base_documents': len(kb.documents),
        'embedding_available': EMBEDDING_AVAILABLE,
        'embedding_model_loaded': kb.embedding_model is not None,
        'optimization_stage': 'stage2_prompt_optimization'
    })

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨ RAG ç³»ç»ŸæœåŠ¡å™¨ - ç¬¬äºŒé˜¶æ®µä¼˜åŒ–ç‰ˆæœ¬")
    print(f"ğŸ“š çŸ¥è¯†åº“æ–‡æ¡£æ•°é‡: {len(kb.documents)}")
    print(f"ğŸ” è¯­ä¹‰æœç´¢åŠŸèƒ½: {'âœ“ å·²å¯ç”¨' if EMBEDDING_AVAILABLE else 'âœ— æœªå¯ç”¨'}")
    print(f"ğŸ¤– æç¤ºè¯ä¼˜åŒ–: âœ“ ç¬¬äºŒé˜¶æ®µä¼˜åŒ–å·²å¯ç”¨")
    app.run(debug=True, host='0.0.0.0', port=5000)
